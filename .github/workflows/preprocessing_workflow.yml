name: Automated Data Preprocessing
on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  schedule:
    - cron: '0 0 * * *'  # Run daily at 00:00 UTC
  workflow_dispatch:  # Manual trigger

jobs:
  preprocessing:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        if [ -f requirements.txt ]; then
          pip install -r requirements.txt
        else
          echo "requirements.txt not found, installing common dependencies"
          pip install pandas numpy scikit-learn joblib
        fi
    
    - name: Create processed_data directory
      run: |
        mkdir -p preprocessing/processed_data
    
    - name: Check file structure
      run: |
        echo "Current directory structure:"
        find . -name "*.py" -o -name "*.csv" | head -20
        echo "Contents of preprocessing directory:"
        ls -la preprocessing/ || echo "preprocessing directory not found"
    
    - name: Run preprocessing
      run: |
        python -c "
        import sys
        import os
        import importlib.util
        
        # Add preprocessing directory to Python path
        preprocessing_dir = os.path.join(os.getcwd(), 'preprocessing')
        sys.path.insert(0, preprocessing_dir)
        
        try:
            # Try to import the module by file path since filename has hyphens
            module_path = os.path.join(preprocessing_dir, 'automate_silmi-azdkiatul-athqia.py')
            if os.path.exists(module_path):
                spec = importlib.util.spec_from_file_location('preprocessing_module', module_path)
                preprocessing_module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(preprocessing_module)
                
                print('üöÄ Starting automated preprocessing...')
                
                # Use the RemoteWorkerDataPreprocessor class
                preprocessor = preprocessing_module.RemoteWorkerDataPreprocessor(random_state=42)
                
                # Check if raw data file exists
                raw_data_path = 'remote_worker_productivity_raw.csv'
                if not os.path.exists(raw_data_path):
                    print(f'‚ùå Raw data file not found: {raw_data_path}')
                    print('Available files in current directory:')
                    for file in os.listdir('.'):
                        if file.endswith('.csv'):
                            print(f'  - {file}')
                    sys.exit(1)
                
                # Run preprocessing
                result = preprocessor.preprocess(
                    file_path=raw_data_path,
                    output_dir='preprocessing/processed_data'
                )
                
                if result and 'X_train' in result:
                    print('‚úÖ Preprocessing completed successfully!')
                    print(f'Training samples: {len(result[\"X_train\"])}')
                    print(f'Validation samples: {len(result[\"X_val\"])}')
                    print(f'Test samples: {len(result[\"X_test\"])}')
                    print(f'Number of features: {result[\"X_train\"].shape[1]}')
                else:
                    print('‚ùå Preprocessing failed - no valid result returned!')
                    sys.exit(1)
                    
            else:
                print(f'‚ùå Preprocessing module not found at: {module_path}')
                print('Available Python files in preprocessing directory:')
                for file in os.listdir(preprocessing_dir):
                    if file.endswith('.py'):
                        print(f'  - {file}')
                sys.exit(1)
                
        except ImportError as e:
            print(f'‚ùå Import error: {e}')
            sys.exit(1)
        except Exception as e:
            print(f'‚ùå Error during preprocessing: {e}')
            import traceback
            traceback.print_exc()
            sys.exit(1)
        "
    
    - name: Verify output files
      run: |
        echo "Checking generated files:"
        if [ -d preprocessing/processed_data ]; then
          echo "Files in processed_data directory:"
          ls -la preprocessing/processed_data/
          
          # Check if key files exist
          for file in data_train.csv data_validation.csv data_test.csv label_encoder.pkl feature_scaler.pkl; do
            if [ -f "preprocessing/processed_data/$file" ]; then
              echo "‚úÖ $file exists"
            else
              echo "‚ùå $file missing"
            fi
          done
        else
          echo "‚ùå processed_data directory not found"
          exit 1
        fi
    
    - name: Upload preprocessing results
      uses: actions/upload-artifact@v4
      if: always()  # Upload artifacts even if previous steps failed
      with:
        name: preprocessed-data-${{ github.run_number }}
        path: |
          preprocessing/processed_data/
          preprocessing/*.pkl
          preprocessing/*.csv
          preprocessing/*.json
        retention-days: 30
    
    - name: Commit and push results
      if: success()  # Only run if preprocessing was successful
      run: |
        git config --local user.email "silmiathqia@gmail.com"
        git config --local user.name "silmiaathqia"
        
        # Check if there are changes to commit
        git add preprocessing/processed_data/
        
        if [ -n "$(git status --porcelain preprocessing/processed_data/)" ]; then
          git commit -m "ü§ñ Auto-update preprocessed data - $(date '+%Y-%m-%d %H:%M:%S')"
          git push
          echo "‚úÖ Changes committed and pushed successfully"
        else
          echo "‚ÑπÔ∏è No changes to commit"
        fi
    
    - name: Create summary comment
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const path = 'preprocessing/processed_data/preprocessing_summary.json';
          
          if (fs.existsSync(path)) {
            const summary = JSON.parse(fs.readFileSync(path, 'utf8'));
            
            const comment = `## ü§ñ Automated Preprocessing Results
            
            ‚úÖ **Preprocessing completed successfully!**
            
            ### Dataset Summary:
            - **Original shape:** ${summary.original_shape ? summary.original_shape.join(' √ó ') : 'N/A'}
            - **Final shape:** ${summary.final_shape_after_validation ? summary.final_shape_after_validation.join(' √ó ') : 'N/A'}
            - **Missing values removed:** ${summary.missing_values_removed || 0}
            - **Duplicates removed:** ${summary.duplicates_removed || 0}
            - **Number of features:** ${summary.num_features || 'N/A'}
            - **Number of classes:** ${summary.num_classes || 'N/A'}
            
            ### Data Split:
            - **Training:** ${summary.split_info ? summary.split_info.train_percentage.toFixed(1) : 'N/A'}%
            - **Validation:** ${summary.split_info ? summary.split_info.val_percentage.toFixed(1) : 'N/A'}%
            - **Test:** ${summary.split_info ? summary.split_info.test_percentage.toFixed(1) : 'N/A'}%
            
            ### Generated Files:
            - \`data_train.csv\`
            - \`data_validation.csv\`
            - \`data_test.csv\`
            - \`label_encoder.pkl\`
            - \`feature_scaler.pkl\`
            - \`preprocessing_summary.json\`
            
            Ready for model training! üöÄ`;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
          }
